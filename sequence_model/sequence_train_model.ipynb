{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b9e4047-44f0-4694-873c-0f57834425d2",
   "metadata": {},
   "source": [
    "## Stage 2 of Image Processing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a269a9-9f32-42ac-b921-b11b74613099",
   "metadata": {},
   "source": [
    "All the chunks for a patient are processed to produce embeddings for each chunk. This is also repeated with an offset of W/2 (12). All the embeddings produced are concatenated and passed to a transformer encoder and linear layer to predict the probability of PE for a patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8ef4cde-1477-41c3-bb2e-c652dccf6d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sequence_dataset import ChunkDataset\n",
    "from sequence_model import CTPAModel\n",
    "from test_model import do_test\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62663c3c-aed8-482d-a23e-0fa6d0b79d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "#reading embeddings with and without offset\n",
    "file_paths0 = [ \"C:\\\\Users\\\\preet\\\\Documents\\\\penet\\\\results\\\\best_result_shift_0\\\\embs_train.pickle\",\n",
    "              \"C:\\\\Users\\\\preet\\\\Documents\\\\penet\\\\results\\\\best_result_shift_0\\\\embs_val.pickle\",\n",
    "              \"C:\\\\Users\\\\preet\\\\Documents\\\\penet\\\\results\\\\best_result_shift_0\\\\embs_test.pickle\"]\n",
    "\n",
    "file_paths1 = [ \"C:\\\\Users\\\\preet\\\\Documents\\\\penet\\\\results\\\\best_result_shift_1\\\\embs_train.pickle\",\n",
    "              \"C:\\\\Users\\\\preet\\\\Documents\\\\penet\\\\results\\\\best_result_shift_1\\\\embs_val.pickle\",\n",
    "              \"C:\\\\Users\\\\preet\\\\Documents\\\\penet\\\\results\\\\best_result_shift_1\\\\embs_test.pickle\"]\n",
    "\n",
    "def return_loaded_files(file_paths):\n",
    "  \n",
    "    with open(file_paths[0], 'rb') as f:\n",
    "            # Load the pickled object from the file\n",
    "            loaded_data_train = pickle.load(f)\n",
    "\n",
    "    with open(file_paths[1], 'rb') as f:\n",
    "            # Load the pickled object from the file\n",
    "            loaded_data_val = pickle.load(f)\n",
    "  \n",
    "    with open(file_paths[2], 'rb') as f:\n",
    "            # Load the pickled object from the file\n",
    "            loaded_data_test = pickle.load(f)\n",
    "    return loaded_data_train,loaded_data_val,loaded_data_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3db2c3e1-16c6-4747-bd08-0f9d44bf1347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['patient', 'slice', 'prob', 'label', 'emb'])\n",
      "24128\n",
      "24128\n",
      "(384,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient</th>\n",
       "      <th>slice</th>\n",
       "      <th>prob</th>\n",
       "      <th>label</th>\n",
       "      <th>emb</th>\n",
       "      <th>emb1</th>\n",
       "      <th>prob1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2506</td>\n",
       "      <td>0</td>\n",
       "      <td>0.049225</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.6442688, 0.25198266, 0.53300434, -0.4185633...</td>\n",
       "      <td>[1.6442688, 0.25198266, 0.53300434, -0.4185633...</td>\n",
       "      <td>0.049225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2506</td>\n",
       "      <td>1</td>\n",
       "      <td>0.052704</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.691245, 0.56891054, 0.23200284, -0.73625606...</td>\n",
       "      <td>[1.5159245, 0.33395997, 0.46993455, -0.5660441...</td>\n",
       "      <td>0.050232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2506</td>\n",
       "      <td>2</td>\n",
       "      <td>0.095337</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.7385237, 0.6826804, -0.18666513, -0.8393175...</td>\n",
       "      <td>[1.8511521, 0.68758553, -0.061858013, -0.73435...</td>\n",
       "      <td>0.067566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2506</td>\n",
       "      <td>3</td>\n",
       "      <td>0.194336</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.5697328, 0.69841456, -0.011741816, -1.05351...</td>\n",
       "      <td>[1.66374, 0.65868104, -0.095052205, -0.9741797...</td>\n",
       "      <td>0.147339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2506</td>\n",
       "      <td>4</td>\n",
       "      <td>0.207275</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.6401, 0.64765483, -0.092930436, -1.0762582,...</td>\n",
       "      <td>[1.5658956, 0.70841485, -0.01152911, -1.120370...</td>\n",
       "      <td>0.214478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2506</td>\n",
       "      <td>5</td>\n",
       "      <td>0.370117</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.3882405, 0.8310233, -0.38720456, -1.4329963...</td>\n",
       "      <td>[1.6777356, 0.668254, -0.110921256, -1.15693, ...</td>\n",
       "      <td>0.220825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2506</td>\n",
       "      <td>6</td>\n",
       "      <td>0.269531</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.6847997, 0.32295343, -0.3549741, -1.1599443...</td>\n",
       "      <td>[1.2737544, 0.6220103, -0.67503995, -1.2819692...</td>\n",
       "      <td>0.427246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2506</td>\n",
       "      <td>7</td>\n",
       "      <td>0.255615</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.720989, 0.5370865, -0.24712141, -1.2112033,...</td>\n",
       "      <td>[1.7898481, 0.38958457, -0.25716192, -1.102011...</td>\n",
       "      <td>0.226807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2506</td>\n",
       "      <td>8</td>\n",
       "      <td>0.312988</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.4537445, 0.7034186, -0.3505595, -1.3398671,...</td>\n",
       "      <td>[1.6212622, 0.62308013, -0.27560562, -1.242563...</td>\n",
       "      <td>0.271729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2506</td>\n",
       "      <td>9</td>\n",
       "      <td>0.221680</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.5071402, 0.71281886, -0.03806496, -1.154002...</td>\n",
       "      <td>[1.3956892, 0.7283744, -0.29031906, -1.3580737...</td>\n",
       "      <td>0.316895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2506</td>\n",
       "      <td>10</td>\n",
       "      <td>0.190918</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.4294186, 0.70495653, 0.07443987, -1.0733806...</td>\n",
       "      <td>[1.5109438, 0.70776105, 0.06505976, -0.9803937...</td>\n",
       "      <td>0.178101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2506</td>\n",
       "      <td>11</td>\n",
       "      <td>0.085876</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.6011457, 0.65436995, 0.19251293, -0.8175314...</td>\n",
       "      <td>[1.5324026, 0.72379744, 0.08257536, -1.0581868...</td>\n",
       "      <td>0.144897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2506</td>\n",
       "      <td>12</td>\n",
       "      <td>0.058228</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.5449529, 0.28860602, 0.30689797, -0.9033593...</td>\n",
       "      <td>[1.562013, 0.4763792, 0.286618, -0.8831149, -0...</td>\n",
       "      <td>0.072388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2506</td>\n",
       "      <td>13</td>\n",
       "      <td>0.067810</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.545089, 0.58123225, 0.44320786, -0.98550355...</td>\n",
       "      <td>[1.4520869, 0.49924508, 0.3110841, -1.1458541,...</td>\n",
       "      <td>0.073303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2506</td>\n",
       "      <td>14</td>\n",
       "      <td>0.047882</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.5668203, 0.3354527, 0.36311096, -0.56665504...</td>\n",
       "      <td>[1.6751659, 0.37031382, 0.47177342, -0.4689444...</td>\n",
       "      <td>0.045105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2506</td>\n",
       "      <td>15</td>\n",
       "      <td>0.043854</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.7800393, 0.1686813, 0.32695946, -0.5912803,...</td>\n",
       "      <td>[1.5000913, 0.20457561, 0.35558182, -0.6316097...</td>\n",
       "      <td>0.047791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2506</td>\n",
       "      <td>16</td>\n",
       "      <td>0.043213</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.8936982, 0.036102623, 0.26092464, -0.478681...</td>\n",
       "      <td>[1.9836702, 0.13913223, 0.3066747, -0.5712142,...</td>\n",
       "      <td>0.045776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2506</td>\n",
       "      <td>17</td>\n",
       "      <td>0.048767</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.8602055, 0.114088096, 0.27197316, -0.512547...</td>\n",
       "      <td>[1.8996779, 0.008689656, 0.17926672, -0.414403...</td>\n",
       "      <td>0.043518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2506</td>\n",
       "      <td>18</td>\n",
       "      <td>0.052826</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.7237082, 0.22913031, 0.4545574, -0.6199504,...</td>\n",
       "      <td>[1.8356823, 0.19879918, 0.3375334, -0.61384416...</td>\n",
       "      <td>0.056854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2506</td>\n",
       "      <td>19</td>\n",
       "      <td>0.059540</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.4981545, 1.0618333, 0.16211443, -0.42043123...</td>\n",
       "      <td>[1.6805947, 0.20859203, 0.49077973, -0.6759074...</td>\n",
       "      <td>0.049774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    patient  slice      prob label  \\\n",
       "0      2506      0  0.049225     0   \n",
       "1      2506      1  0.052704     0   \n",
       "2      2506      2  0.095337     0   \n",
       "3      2506      3  0.194336     0   \n",
       "4      2506      4  0.207275     0   \n",
       "5      2506      5  0.370117     0   \n",
       "6      2506      6  0.269531     0   \n",
       "7      2506      7  0.255615     0   \n",
       "8      2506      8  0.312988     0   \n",
       "9      2506      9  0.221680     0   \n",
       "10     2506     10  0.190918     0   \n",
       "11     2506     11  0.085876     0   \n",
       "12     2506     12  0.058228     0   \n",
       "13     2506     13  0.067810     0   \n",
       "14     2506     14  0.047882     0   \n",
       "15     2506     15  0.043854     0   \n",
       "16     2506     16  0.043213     0   \n",
       "17     2506     17  0.048767     0   \n",
       "18     2506     18  0.052826     0   \n",
       "19     2506     19  0.059540     0   \n",
       "\n",
       "                                                  emb  \\\n",
       "0   [1.6442688, 0.25198266, 0.53300434, -0.4185633...   \n",
       "1   [1.691245, 0.56891054, 0.23200284, -0.73625606...   \n",
       "2   [1.7385237, 0.6826804, -0.18666513, -0.8393175...   \n",
       "3   [1.5697328, 0.69841456, -0.011741816, -1.05351...   \n",
       "4   [1.6401, 0.64765483, -0.092930436, -1.0762582,...   \n",
       "5   [1.3882405, 0.8310233, -0.38720456, -1.4329963...   \n",
       "6   [1.6847997, 0.32295343, -0.3549741, -1.1599443...   \n",
       "7   [1.720989, 0.5370865, -0.24712141, -1.2112033,...   \n",
       "8   [1.4537445, 0.7034186, -0.3505595, -1.3398671,...   \n",
       "9   [1.5071402, 0.71281886, -0.03806496, -1.154002...   \n",
       "10  [1.4294186, 0.70495653, 0.07443987, -1.0733806...   \n",
       "11  [1.6011457, 0.65436995, 0.19251293, -0.8175314...   \n",
       "12  [1.5449529, 0.28860602, 0.30689797, -0.9033593...   \n",
       "13  [1.545089, 0.58123225, 0.44320786, -0.98550355...   \n",
       "14  [1.5668203, 0.3354527, 0.36311096, -0.56665504...   \n",
       "15  [1.7800393, 0.1686813, 0.32695946, -0.5912803,...   \n",
       "16  [1.8936982, 0.036102623, 0.26092464, -0.478681...   \n",
       "17  [1.8602055, 0.114088096, 0.27197316, -0.512547...   \n",
       "18  [1.7237082, 0.22913031, 0.4545574, -0.6199504,...   \n",
       "19  [1.4981545, 1.0618333, 0.16211443, -0.42043123...   \n",
       "\n",
       "                                                 emb1     prob1  \n",
       "0   [1.6442688, 0.25198266, 0.53300434, -0.4185633...  0.049225  \n",
       "1   [1.5159245, 0.33395997, 0.46993455, -0.5660441...  0.050232  \n",
       "2   [1.8511521, 0.68758553, -0.061858013, -0.73435...  0.067566  \n",
       "3   [1.66374, 0.65868104, -0.095052205, -0.9741797...  0.147339  \n",
       "4   [1.5658956, 0.70841485, -0.01152911, -1.120370...  0.214478  \n",
       "5   [1.6777356, 0.668254, -0.110921256, -1.15693, ...  0.220825  \n",
       "6   [1.2737544, 0.6220103, -0.67503995, -1.2819692...  0.427246  \n",
       "7   [1.7898481, 0.38958457, -0.25716192, -1.102011...  0.226807  \n",
       "8   [1.6212622, 0.62308013, -0.27560562, -1.242563...  0.271729  \n",
       "9   [1.3956892, 0.7283744, -0.29031906, -1.3580737...  0.316895  \n",
       "10  [1.5109438, 0.70776105, 0.06505976, -0.9803937...  0.178101  \n",
       "11  [1.5324026, 0.72379744, 0.08257536, -1.0581868...  0.144897  \n",
       "12  [1.562013, 0.4763792, 0.286618, -0.8831149, -0...  0.072388  \n",
       "13  [1.4520869, 0.49924508, 0.3110841, -1.1458541,...  0.073303  \n",
       "14  [1.6751659, 0.37031382, 0.47177342, -0.4689444...  0.045105  \n",
       "15  [1.5000913, 0.20457561, 0.35558182, -0.6316097...  0.047791  \n",
       "16  [1.9836702, 0.13913223, 0.3066747, -0.5712142,...  0.045776  \n",
       "17  [1.8996779, 0.008689656, 0.17926672, -0.414403...  0.043518  \n",
       "18  [1.8356823, 0.19879918, 0.3375334, -0.61384416...  0.056854  \n",
       "19  [1.6805947, 0.20859203, 0.49077973, -0.6759074...  0.049774  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create one dataframe with the embeddings with and without offsent for each slice for a patient as well as the probability of PE for each slice\n",
    "\n",
    "\n",
    "emb_train, emb_val, emb_test = return_loaded_files(file_paths0)\n",
    "emb_train1, emb_val1, emb_test1 = return_loaded_files(file_paths1)\n",
    "print(emb_train.keys())\n",
    "print(len(emb_train['patient']))\n",
    "print(len(emb_train1['patient']))\n",
    "print(emb_train['emb'][0].shape)\n",
    "\n",
    "df_train = pd.DataFrame(emb_train)\n",
    "df_train[\"emb1\"] = emb_train1[\"emb\"]\n",
    "df_train[\"prob1\"] = emb_train1[\"prob\"]\n",
    "df_val = pd.DataFrame(emb_val)\n",
    "df_val[\"emb1\"] = emb_val1[\"emb\"]\n",
    "df_val[\"prob1\"] = emb_val1[\"prob\"]\n",
    "\n",
    "df_test = pd.DataFrame(emb_test)\n",
    "df_test[\"emb1\"] = emb_test1[\"emb\"]\n",
    "df_test[\"prob1\"] = emb_test1[\"prob\"]\n",
    "\n",
    "df_test.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a4ebd7-3e15-4408-ab60-eace927be397",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dataset = ChunkDataset(df_train)\n",
    "val_dataset = ChunkDataset(df_val)\n",
    "test_dataset = ChunkDataset(df_test)\n",
    "train_dataloader = DataLoader(trn_dataset, batch_size=4, num_workers=0,shuffle=True,drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=2, num_workers=0,drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=2, num_workers=0,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4d3d7b4f-c349-4b8d-bb85-678960a9daab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 11.23533\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 384\n",
    "num_heads = 8\n",
    "num_layers = 4\n",
    "\n",
    "model = CTPAModel(embed_dim, num_heads, num_layers, drop_prob=0.1)\n",
    "# Alternatively, to just get the total parameter count:\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params/1e6}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04364486-2de1-4c6c-8753-e57e4e3e6804",
   "metadata": {},
   "source": [
    "## Train and Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "70689b35-059c-4cf8-9785-283e6d6cf2f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\preet\\AppData\\Local\\Temp\\ipykernel_21304\\4024281372.py:28: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 357/357 [00:13<00:00, 27.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Train loss0.486, F1 0.700, Acc 0.767, BA 0.686, precision 0.773, recall 0.438\n",
      "cm[[884  62]\n",
      " [271 211]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\preet\\AppData\\Local\\Temp\\ipykernel_21304\\2734414312.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output.extend(F.softmax(outputs).cpu().numpy()[:, 1])\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 82/82 [00:00<00:00, 107.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss0.471, F1 0.720, Acc 0.799, BA 0.794, precision 0.449, recall 0.786\n",
      "cm[[109  27]\n",
      " [  6  22]]\n",
      "Val AUC-ROC: 0.8534663865546218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 357/357 [00:12<00:00, 28.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n",
      "Train loss0.293, F1 0.871, Acc 0.888, BA 0.860, precision 0.880, recall 0.774\n",
      "cm[[894  51]\n",
      " [109 374]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\preet\\AppData\\Local\\Temp\\ipykernel_21304\\2734414312.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output.extend(F.softmax(outputs).cpu().numpy()[:, 1])\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 82/82 [00:00<00:00, 104.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss0.628, F1 0.696, Acc 0.768, BA 0.789, precision 0.411, recall 0.821\n",
      "cm[[103  33]\n",
      " [  5  23]]\n",
      "Val AUC-ROC: 0.8550420168067226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 357/357 [00:12<00:00, 28.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n",
      "Train loss0.274, F1 0.886, Acc 0.901, BA 0.876, precision 0.898, recall 0.799\n",
      "cm[[901  44]\n",
      " [ 97 386]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\preet\\AppData\\Local\\Temp\\ipykernel_21304\\2734414312.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output.extend(F.softmax(outputs).cpu().numpy()[:, 1])\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 82/82 [00:00<00:00, 104.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss0.592, F1 0.731, Acc 0.805, BA 0.811, precision 0.460, recall 0.821\n",
      "cm[[109  27]\n",
      " [  5  23]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 357/357 [00:12<00:00, 28.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n",
      "Train loss0.281, F1 0.884, Acc 0.900, BA 0.872, precision 0.905, recall 0.786\n",
      "cm[[906  40]\n",
      " [103 379]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\preet\\AppData\\Local\\Temp\\ipykernel_21304\\2734414312.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output.extend(F.softmax(outputs).cpu().numpy()[:, 1])\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 82/82 [00:00<00:00, 104.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss0.570, F1 0.707, Acc 0.780, BA 0.797, precision 0.426, recall 0.821\n",
      "cm[[105  31]\n",
      " [  5  23]]\n",
      "Val AUC-ROC: 0.8555672268907564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 357/357 [00:12<00:00, 28.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n",
      "Train loss0.267, F1 0.896, Acc 0.910, BA 0.884, precision 0.919, recall 0.803\n",
      "cm[[911  34]\n",
      " [ 95 388]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\preet\\AppData\\Local\\Temp\\ipykernel_21304\\2734414312.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output.extend(F.softmax(outputs).cpu().numpy()[:, 1])\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 82/82 [00:00<00:00, 101.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss0.428, F1 0.784, Acc 0.860, BA 0.830, precision 0.564, recall 0.786\n",
      "cm[[119  17]\n",
      " [  6  22]]\n",
      "Val AUC-ROC: 0.8568802521008404\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score,balanced_accuracy_score, precision_score, recall_score\n",
    "import torch\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, RocCurveDisplay\n",
    "import os\n",
    "from sklearn.metrics import average_precision_score\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer (Adam)\n",
    "learning_rate = 1e-5\n",
    "num_epochs = 5\n",
    "model = CTPAModel(embed_dim, num_heads, num_layers,drop_prob=0.2)\n",
    "\n",
    "model = model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR   # or CosineAnnealingWarmRestarts\n",
    "\n",
    "min_lr   = 1e-6                      # floor value\n",
    "scheduler = CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=num_epochs,                # # of scheduler updates before it restarts (here: 1 cycle = whole run)\n",
    "    eta_min=min_lr                   # lowest LR the cosine will hit\n",
    ")\n",
    "use_amp = True\n",
    "device = \"cuda\"\n",
    "thresh = 0\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_rocs=[]\n",
    "test_losses = []\n",
    "test_rocs=[]\n",
    "best = 0\n",
    "for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        total_accuracy = 0.0\n",
    "        num_train_batches = len(train_dataloader)\n",
    "        \n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        f1_scores =[]\n",
    "        \n",
    "        for num_batch,batch in enumerate(tqdm(train_dataloader, total=num_train_batches)):\n",
    "            if num_batch < num_train_batches:\n",
    "                with torch.autocast(device_type=device, dtype=torch.float16, enabled=use_amp):\n",
    "                    inputs = batch[0] #.to(device)\n",
    "                    \n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = torch.squeeze(batch[1].to(device))\n",
    "                   # print(labels)\n",
    "                    #M = (torch.rand(inputs.size()) > thresh).to(torch.long)\n",
    "                    #M = M.cuda()\n",
    "                    #print(M)\n",
    "                    #One = torch.ones(inputs.size()).cuda()\n",
    "                    #inputs = M * inputs + (One-M)*(One-inputs)\n",
    "                    outputs,_ = model(inputs) #.to(torch.long))\n",
    "                  #  print(outputs)\n",
    "    \n",
    "                   # print(outputs.size(), labels.size())\n",
    "                    #print(outputs.shape, labels.shape)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    preds = torch.argmax(outputs, dim=1) \n",
    "                    all_preds.extend(preds.cpu().numpy())\n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scheduler.step()\n",
    "                scaler.update()\n",
    "                \n",
    "                optimizer.zero_grad(set_to_none=True) # set_to_none=True here can modestly improve performance\n",
    "                total_loss += loss.item()\n",
    "            #if num_batch == 100:\n",
    "            #    end_timer_and_print(\"Test\")\n",
    "                \n",
    "       # end_timer_and_print(\"Test\")\n",
    "\n",
    "        avg_train_loss = total_loss / num_train_batches\n",
    "        train_losses.append(avg_train_loss)\n",
    "        f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        acc = accuracy_score(all_labels, all_preds)\n",
    "        ba = balanced_accuracy_score(all_labels, all_preds)\n",
    "      \n",
    "        precision = precision_score(all_labels, all_preds)\n",
    "        recall = recall_score(all_labels, all_preds)\n",
    "        f1_scores.append(f1)\n",
    "        confusion_matrix_sc = confusion_matrix(all_labels, all_preds)\n",
    "        print(f'Epoch {epoch + 1}')\n",
    "        print(\"Train loss{b:.3f}, F1 {c:.3f}, Acc {d:.3f}, BA {e:.3f}, precision {f:.3f}, recall {g:.3f}\".format( b=avg_train_loss, c=f1, d = acc, e= ba, f=precision, g=recall))\n",
    "        print(f\"cm{confusion_matrix_sc}\") \n",
    "        model.eval()\n",
    "        \n",
    "        #x = do_test(val_dataloader)\n",
    "        #val_losses.append(x.item())\n",
    "        save_dir = r\"C:\\Users\\preet\\Documents\\penet\\results_sequence\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        # Evaluation\n",
    "        metrics, output = do_test(val_dataloader, model)\n",
    "        roc_auc = metrics[\"AUC-ROC\"]\n",
    "        \n",
    "        # Save if best\n",
    "        if roc_auc >= best:\n",
    "            best = roc_auc\n",
    "            print(\"Val AUC-ROC:\", roc_auc)\n",
    "            val_rocs.append(roc_auc)\n",
    "            fname = os.path.join(save_dir, \"best_model.pth\")\n",
    "            torch.save(model.state_dict(), fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "e8abb0bd-b6de-486a-a9c6-4f42b82a7cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metrics(result_df):\n",
    "    from sklearn.metrics import (\n",
    "            accuracy_score, precision_score, recall_score, f1_score,\n",
    "            roc_auc_score, average_precision_score, balanced_accuracy_score,\n",
    "            confusion_matrix\n",
    "        )\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Get true labels and predicted probabilities\n",
    "    y_true = result_df['label'].values\n",
    "    y_prob = result_df['final_prob'].values\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "    \n",
    "    # Confusion matrix to get TN, FP, FN, TP\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    # Compute metrics\n",
    "    metrics = {\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Sensitivity\": recall_score(y_true, y_pred),  # aka recall\n",
    "        \"Specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0.0,\n",
    "        \"Precision\": precision_score(y_true, y_pred),\n",
    "        \"F1 Score\": f1_score(y_true, y_pred),\n",
    "        \"AUC-ROC\": roc_auc_score(y_true, y_prob),\n",
    "        \"AUC-PR\": average_precision_score(y_true, y_prob),\n",
    "        \"Balanced Accuracy\": balanced_accuracy_score(y_true, y_pred)\n",
    "    }\n",
    "        \n",
    "    # Add to metrics_df\n",
    "    metrics_df2 = pd.DataFrame([metrics])  # single row\n",
    "    return metrics_df2 \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
