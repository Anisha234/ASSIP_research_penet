{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b9e4047-44f0-4694-873c-0f57834425d2",
   "metadata": {},
   "source": [
    "## Stage 2 of Image Processing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a269a9-9f32-42ac-b921-b11b74613099",
   "metadata": {},
   "source": [
    "All the chunks for a patient are processed to produce embeddings for each chunk. This is also repeated with an offset of W/2 (12). All the embeddings produced are concatenated and passed to a transformer encoder and linear layer to predict the probability of PE for a patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62663c3c-aed8-482d-a23e-0fa6d0b79d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "#reading embeddings with and without offset\n",
    "file_paths0 = [ \"C:\\\\Users\\\\preet\\\\Documents\\\\penet\\\\results\\\\best_result_shift_0\\\\embs_train.pickle\",\n",
    "              \"C:\\\\Users\\\\preet\\\\Documents\\\\penet\\\\results\\\\best_result_shift_0\\\\embs_val.pickle\",\n",
    "              \"C:\\\\Users\\\\preet\\\\Documents\\\\penet\\\\results\\\\best_result_shift_0\\\\embs_test.pickle\"]\n",
    "\n",
    "file_paths1 = [ \"C:\\\\Users\\\\preet\\\\Documents\\\\penet\\\\results\\\\best_result_shift_1\\\\embs_train.pickle\",\n",
    "              \"C:\\\\Users\\\\preet\\\\Documents\\\\penet\\\\results\\\\best_result_shift_1\\\\embs_val.pickle\",\n",
    "              \"C:\\\\Users\\\\preet\\\\Documents\\\\penet\\\\results\\\\best_result_shift_1\\\\embs_test.pickle\"]\n",
    "\n",
    "def return_loaded_files(file_paths):\n",
    "  \n",
    "    with open(file_paths[0], 'rb') as f:\n",
    "            # Load the pickled object from the file\n",
    "            loaded_data_train = pickle.load(f)\n",
    "\n",
    "    with open(file_paths[1], 'rb') as f:\n",
    "            # Load the pickled object from the file\n",
    "            loaded_data_val = pickle.load(f)\n",
    "  \n",
    "    with open(file_paths[2], 'rb') as f:\n",
    "            # Load the pickled object from the file\n",
    "            loaded_data_test = pickle.load(f)\n",
    "    return loaded_data_train,loaded_data_val,loaded_data_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3db2c3e1-16c6-4747-bd08-0f9d44bf1347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['patient', 'slice', 'prob', 'label', 'emb'])\n",
      "24128\n",
      "24128\n",
      "(384,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient</th>\n",
       "      <th>slice</th>\n",
       "      <th>prob</th>\n",
       "      <th>label</th>\n",
       "      <th>emb</th>\n",
       "      <th>emb1</th>\n",
       "      <th>prob1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2506</td>\n",
       "      <td>0</td>\n",
       "      <td>0.049225</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.6442688, 0.25198266, 0.53300434, -0.4185633...</td>\n",
       "      <td>[1.6442688, 0.25198266, 0.53300434, -0.4185633...</td>\n",
       "      <td>0.049225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2506</td>\n",
       "      <td>1</td>\n",
       "      <td>0.052704</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.691245, 0.56891054, 0.23200284, -0.73625606...</td>\n",
       "      <td>[1.5159245, 0.33395997, 0.46993455, -0.5660441...</td>\n",
       "      <td>0.050232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2506</td>\n",
       "      <td>2</td>\n",
       "      <td>0.095337</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.7385237, 0.6826804, -0.18666513, -0.8393175...</td>\n",
       "      <td>[1.8511521, 0.68758553, -0.061858013, -0.73435...</td>\n",
       "      <td>0.067566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2506</td>\n",
       "      <td>3</td>\n",
       "      <td>0.194336</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.5697328, 0.69841456, -0.011741816, -1.05351...</td>\n",
       "      <td>[1.66374, 0.65868104, -0.095052205, -0.9741797...</td>\n",
       "      <td>0.147339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2506</td>\n",
       "      <td>4</td>\n",
       "      <td>0.207275</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.6401, 0.64765483, -0.092930436, -1.0762582,...</td>\n",
       "      <td>[1.5658956, 0.70841485, -0.01152911, -1.120370...</td>\n",
       "      <td>0.214478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2506</td>\n",
       "      <td>5</td>\n",
       "      <td>0.370117</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.3882405, 0.8310233, -0.38720456, -1.4329963...</td>\n",
       "      <td>[1.6777356, 0.668254, -0.110921256, -1.15693, ...</td>\n",
       "      <td>0.220825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2506</td>\n",
       "      <td>6</td>\n",
       "      <td>0.269531</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.6847997, 0.32295343, -0.3549741, -1.1599443...</td>\n",
       "      <td>[1.2737544, 0.6220103, -0.67503995, -1.2819692...</td>\n",
       "      <td>0.427246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2506</td>\n",
       "      <td>7</td>\n",
       "      <td>0.255615</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.720989, 0.5370865, -0.24712141, -1.2112033,...</td>\n",
       "      <td>[1.7898481, 0.38958457, -0.25716192, -1.102011...</td>\n",
       "      <td>0.226807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2506</td>\n",
       "      <td>8</td>\n",
       "      <td>0.312988</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.4537445, 0.7034186, -0.3505595, -1.3398671,...</td>\n",
       "      <td>[1.6212622, 0.62308013, -0.27560562, -1.242563...</td>\n",
       "      <td>0.271729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2506</td>\n",
       "      <td>9</td>\n",
       "      <td>0.221680</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.5071402, 0.71281886, -0.03806496, -1.154002...</td>\n",
       "      <td>[1.3956892, 0.7283744, -0.29031906, -1.3580737...</td>\n",
       "      <td>0.316895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2506</td>\n",
       "      <td>10</td>\n",
       "      <td>0.190918</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.4294186, 0.70495653, 0.07443987, -1.0733806...</td>\n",
       "      <td>[1.5109438, 0.70776105, 0.06505976, -0.9803937...</td>\n",
       "      <td>0.178101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2506</td>\n",
       "      <td>11</td>\n",
       "      <td>0.085876</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.6011457, 0.65436995, 0.19251293, -0.8175314...</td>\n",
       "      <td>[1.5324026, 0.72379744, 0.08257536, -1.0581868...</td>\n",
       "      <td>0.144897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2506</td>\n",
       "      <td>12</td>\n",
       "      <td>0.058228</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.5449529, 0.28860602, 0.30689797, -0.9033593...</td>\n",
       "      <td>[1.562013, 0.4763792, 0.286618, -0.8831149, -0...</td>\n",
       "      <td>0.072388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2506</td>\n",
       "      <td>13</td>\n",
       "      <td>0.067810</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.545089, 0.58123225, 0.44320786, -0.98550355...</td>\n",
       "      <td>[1.4520869, 0.49924508, 0.3110841, -1.1458541,...</td>\n",
       "      <td>0.073303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2506</td>\n",
       "      <td>14</td>\n",
       "      <td>0.047882</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.5668203, 0.3354527, 0.36311096, -0.56665504...</td>\n",
       "      <td>[1.6751659, 0.37031382, 0.47177342, -0.4689444...</td>\n",
       "      <td>0.045105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2506</td>\n",
       "      <td>15</td>\n",
       "      <td>0.043854</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.7800393, 0.1686813, 0.32695946, -0.5912803,...</td>\n",
       "      <td>[1.5000913, 0.20457561, 0.35558182, -0.6316097...</td>\n",
       "      <td>0.047791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2506</td>\n",
       "      <td>16</td>\n",
       "      <td>0.043213</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.8936982, 0.036102623, 0.26092464, -0.478681...</td>\n",
       "      <td>[1.9836702, 0.13913223, 0.3066747, -0.5712142,...</td>\n",
       "      <td>0.045776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2506</td>\n",
       "      <td>17</td>\n",
       "      <td>0.048767</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.8602055, 0.114088096, 0.27197316, -0.512547...</td>\n",
       "      <td>[1.8996779, 0.008689656, 0.17926672, -0.414403...</td>\n",
       "      <td>0.043518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2506</td>\n",
       "      <td>18</td>\n",
       "      <td>0.052826</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.7237082, 0.22913031, 0.4545574, -0.6199504,...</td>\n",
       "      <td>[1.8356823, 0.19879918, 0.3375334, -0.61384416...</td>\n",
       "      <td>0.056854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2506</td>\n",
       "      <td>19</td>\n",
       "      <td>0.059540</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.4981545, 1.0618333, 0.16211443, -0.42043123...</td>\n",
       "      <td>[1.6805947, 0.20859203, 0.49077973, -0.6759074...</td>\n",
       "      <td>0.049774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    patient  slice      prob label  \\\n",
       "0      2506      0  0.049225     0   \n",
       "1      2506      1  0.052704     0   \n",
       "2      2506      2  0.095337     0   \n",
       "3      2506      3  0.194336     0   \n",
       "4      2506      4  0.207275     0   \n",
       "5      2506      5  0.370117     0   \n",
       "6      2506      6  0.269531     0   \n",
       "7      2506      7  0.255615     0   \n",
       "8      2506      8  0.312988     0   \n",
       "9      2506      9  0.221680     0   \n",
       "10     2506     10  0.190918     0   \n",
       "11     2506     11  0.085876     0   \n",
       "12     2506     12  0.058228     0   \n",
       "13     2506     13  0.067810     0   \n",
       "14     2506     14  0.047882     0   \n",
       "15     2506     15  0.043854     0   \n",
       "16     2506     16  0.043213     0   \n",
       "17     2506     17  0.048767     0   \n",
       "18     2506     18  0.052826     0   \n",
       "19     2506     19  0.059540     0   \n",
       "\n",
       "                                                  emb  \\\n",
       "0   [1.6442688, 0.25198266, 0.53300434, -0.4185633...   \n",
       "1   [1.691245, 0.56891054, 0.23200284, -0.73625606...   \n",
       "2   [1.7385237, 0.6826804, -0.18666513, -0.8393175...   \n",
       "3   [1.5697328, 0.69841456, -0.011741816, -1.05351...   \n",
       "4   [1.6401, 0.64765483, -0.092930436, -1.0762582,...   \n",
       "5   [1.3882405, 0.8310233, -0.38720456, -1.4329963...   \n",
       "6   [1.6847997, 0.32295343, -0.3549741, -1.1599443...   \n",
       "7   [1.720989, 0.5370865, -0.24712141, -1.2112033,...   \n",
       "8   [1.4537445, 0.7034186, -0.3505595, -1.3398671,...   \n",
       "9   [1.5071402, 0.71281886, -0.03806496, -1.154002...   \n",
       "10  [1.4294186, 0.70495653, 0.07443987, -1.0733806...   \n",
       "11  [1.6011457, 0.65436995, 0.19251293, -0.8175314...   \n",
       "12  [1.5449529, 0.28860602, 0.30689797, -0.9033593...   \n",
       "13  [1.545089, 0.58123225, 0.44320786, -0.98550355...   \n",
       "14  [1.5668203, 0.3354527, 0.36311096, -0.56665504...   \n",
       "15  [1.7800393, 0.1686813, 0.32695946, -0.5912803,...   \n",
       "16  [1.8936982, 0.036102623, 0.26092464, -0.478681...   \n",
       "17  [1.8602055, 0.114088096, 0.27197316, -0.512547...   \n",
       "18  [1.7237082, 0.22913031, 0.4545574, -0.6199504,...   \n",
       "19  [1.4981545, 1.0618333, 0.16211443, -0.42043123...   \n",
       "\n",
       "                                                 emb1     prob1  \n",
       "0   [1.6442688, 0.25198266, 0.53300434, -0.4185633...  0.049225  \n",
       "1   [1.5159245, 0.33395997, 0.46993455, -0.5660441...  0.050232  \n",
       "2   [1.8511521, 0.68758553, -0.061858013, -0.73435...  0.067566  \n",
       "3   [1.66374, 0.65868104, -0.095052205, -0.9741797...  0.147339  \n",
       "4   [1.5658956, 0.70841485, -0.01152911, -1.120370...  0.214478  \n",
       "5   [1.6777356, 0.668254, -0.110921256, -1.15693, ...  0.220825  \n",
       "6   [1.2737544, 0.6220103, -0.67503995, -1.2819692...  0.427246  \n",
       "7   [1.7898481, 0.38958457, -0.25716192, -1.102011...  0.226807  \n",
       "8   [1.6212622, 0.62308013, -0.27560562, -1.242563...  0.271729  \n",
       "9   [1.3956892, 0.7283744, -0.29031906, -1.3580737...  0.316895  \n",
       "10  [1.5109438, 0.70776105, 0.06505976, -0.9803937...  0.178101  \n",
       "11  [1.5324026, 0.72379744, 0.08257536, -1.0581868...  0.144897  \n",
       "12  [1.562013, 0.4763792, 0.286618, -0.8831149, -0...  0.072388  \n",
       "13  [1.4520869, 0.49924508, 0.3110841, -1.1458541,...  0.073303  \n",
       "14  [1.6751659, 0.37031382, 0.47177342, -0.4689444...  0.045105  \n",
       "15  [1.5000913, 0.20457561, 0.35558182, -0.6316097...  0.047791  \n",
       "16  [1.9836702, 0.13913223, 0.3066747, -0.5712142,...  0.045776  \n",
       "17  [1.8996779, 0.008689656, 0.17926672, -0.414403...  0.043518  \n",
       "18  [1.8356823, 0.19879918, 0.3375334, -0.61384416...  0.056854  \n",
       "19  [1.6805947, 0.20859203, 0.49077973, -0.6759074...  0.049774  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create one dataframe with the embeddings with and without offsent for each slice for a patient as well as the probability of PE for each slice\n",
    "\n",
    "emb_train, emb_val, emb_test = return_loaded_files(file_paths0)\n",
    "emb_train1, emb_val1, emb_test1 = return_loaded_files(file_paths1)\n",
    "print(emb_train.keys())\n",
    "print(len(emb_train['patient']))\n",
    "print(len(emb_train1['patient']))\n",
    "print(emb_train['emb'][0].shape)\n",
    "\n",
    "df_train = pd.DataFrame(emb_train)\n",
    "df_train[\"emb1\"] = emb_train1[\"emb\"]\n",
    "df_train[\"prob1\"] = emb_train1[\"prob\"]\n",
    "df_val = pd.DataFrame(emb_val)\n",
    "df_val[\"emb1\"] = emb_val1[\"emb\"]\n",
    "df_val[\"prob1\"] = emb_val1[\"prob\"]\n",
    "\n",
    "df_test = pd.DataFrame(emb_test)\n",
    "df_test[\"emb1\"] = emb_test1[\"emb\"]\n",
    "df_test[\"prob1\"] = emb_test1[\"prob\"]\n",
    "\n",
    "df_test.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60b59b42-d173-4433-bd97-56c951c83828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1429"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ChunkDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        super(ChunkDataset, self).__init__()\n",
    "\n",
    "        self.df = df\n",
    "        self.pat_list = list(set(self.df[\"patient\"]))\n",
    "      \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(list(set(self.df[\"patient\"])))\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        pat = self.pat_list[i]\n",
    "  \n",
    "        patient_list = list((self.df['patient']))\n",
    "        idx = patient_list.index(pat)\n",
    "  \n",
    "        emblist = []\n",
    "        while idx<len(patient_list) and (self.df['patient'][idx]) == pat:\n",
    "            emblist.append(self.df[\"emb\"][idx])\n",
    "            if \"emb1\" in self.df.columns:\n",
    "                emblist.append(self.df[\"emb1\"][idx])\n",
    "            idx+=1\n",
    "        x =torch.tensor(emblist) \n",
    "        M = 256\n",
    "        D = x.shape[1]\n",
    "        S = x.shape[0]\n",
    "        idx = patient_list.index(pat)\n",
    "        \n",
    "        padded = torch.zeros(\n",
    "            M, D,\n",
    "            dtype=x.dtype,\n",
    "            device=x.device\n",
    "        )\n",
    "        \n",
    "        \n",
    "        padded[:S] = x\n",
    "     \n",
    "        return padded, int(self.df[\"label\"][idx]), int(pat)\n",
    "\n",
    "trn_dataset = ChunkDataset(df_train)\n",
    "val_dataset = ChunkDataset(df_val)\n",
    "test_dataset = ChunkDataset(df_test)\n",
    "\n",
    "len(trn_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb809823-8fd7-4d03-ab56-7ba839141c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient</th>\n",
       "      <th>slice</th>\n",
       "      <th>prob</th>\n",
       "      <th>label</th>\n",
       "      <th>emb</th>\n",
       "      <th>emb1</th>\n",
       "      <th>prob1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1436</td>\n",
       "      <td>0</td>\n",
       "      <td>0.057190</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.7599375, 0.5534028, 0.39009404, -0.59744406...</td>\n",
       "      <td>[1.7599375, 0.5534028, 0.39009404, -0.59744406...</td>\n",
       "      <td>0.057190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1436</td>\n",
       "      <td>1</td>\n",
       "      <td>0.060516</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.813671, 0.55590856, 0.30155247, -0.680811, ...</td>\n",
       "      <td>[1.8462007, 0.5318683, 0.2710509, -0.6250644, ...</td>\n",
       "      <td>0.065247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1436</td>\n",
       "      <td>2</td>\n",
       "      <td>0.053192</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.800582, 0.560895, 0.37443057, -0.89901614, ...</td>\n",
       "      <td>[1.7651831, 0.63981956, 0.43286446, -0.9088285...</td>\n",
       "      <td>0.058777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1436</td>\n",
       "      <td>3</td>\n",
       "      <td>0.080933</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.6854982, 0.5283177, 0.21346918, -0.96257645...</td>\n",
       "      <td>[1.8423945, 0.42081487, 0.35151595, -0.8278398...</td>\n",
       "      <td>0.048950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1436</td>\n",
       "      <td>4</td>\n",
       "      <td>0.302246</td>\n",
       "      <td>0</td>\n",
       "      <td>[1.1685085, 1.0288838, -0.07102979, -1.3111601...</td>\n",
       "      <td>[1.4185561, 0.71315014, 0.13641657, -1.202428,...</td>\n",
       "      <td>0.172974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   patient  slice      prob label  \\\n",
       "0     1436      0  0.057190     0   \n",
       "1     1436      1  0.060516     0   \n",
       "2     1436      2  0.053192     0   \n",
       "3     1436      3  0.080933     0   \n",
       "4     1436      4  0.302246     0   \n",
       "\n",
       "                                                 emb  \\\n",
       "0  [1.7599375, 0.5534028, 0.39009404, -0.59744406...   \n",
       "1  [1.813671, 0.55590856, 0.30155247, -0.680811, ...   \n",
       "2  [1.800582, 0.560895, 0.37443057, -0.89901614, ...   \n",
       "3  [1.6854982, 0.5283177, 0.21346918, -0.96257645...   \n",
       "4  [1.1685085, 1.0288838, -0.07102979, -1.3111601...   \n",
       "\n",
       "                                                emb1     prob1  \n",
       "0  [1.7599375, 0.5534028, 0.39009404, -0.59744406...  0.057190  \n",
       "1  [1.8462007, 0.5318683, 0.2710509, -0.6250644, ...  0.065247  \n",
       "2  [1.7651831, 0.63981956, 0.43286446, -0.9088285...  0.058777  \n",
       "3  [1.8423945, 0.42081487, 0.35151595, -0.8278398...  0.048950  \n",
       "4  [1.4185561, 0.71315014, 0.13641657, -1.202428,...  0.172974  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5919252c-d0db-4b99-812d-3890b952dd74",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "87fcad0e-a70c-4dcf-8302-06046f5b52a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(trn_dataset, batch_size=4, num_workers=0,shuffle=True,drop_last=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=2, num_workers=0,drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=2, num_workers=0,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "193c3ebf-0fae-4c75-a79a-5bfea75a9bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CTPAModel(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, num_layers,drop_prob = 0.0, max_len=1024):\n",
    "        super(CTPAModel, self).__init__()\n",
    "        \n",
    "        self.cls_emb  = torch.nn.Parameter(torch.randn(1, 1,embed_dim))\n",
    "        #self.cls_emb.requires_grad_(False)  \n",
    "        self.embed_dim = embed_dim\n",
    "          # Positional embedding: [max_len, embed_dim]\n",
    "        self.pos_embedding = nn.Embedding(max_len, embed_dim)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.embed_dim, \n",
    "            nhead=num_heads, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer=self.encoder_layer,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        \n",
    "        self.output_linear = nn.Linear(self.embed_dim, 2)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.drop_prob = drop_prob\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # print(len(x_list), x_list[0].size()) #2 x 3 x 512 x 512\n",
    "        \n",
    "         \n",
    "        if self.training:\n",
    "            b,s,d = x.size()\n",
    "            keep_mask = torch.rand(s) > self.drop_prob  # shape: (s,)\n",
    "            \n",
    "        #    print(x.size())\n",
    "            # Apply mask across sequence dimension\n",
    "            x = x[:, keep_mask, :]  # shape: (b, s', d) where s' <= s\n",
    "        #    print(\"Mask\",keep_mask)\n",
    "        #    print(x.size())\n",
    "        \n",
    "        cls_tokens = self.cls_emb.expand(x.size()[0], -1, -1)  # (batch_size, 1, embed_dim)\n",
    "        #x = self.dropout(x)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        B,L,D = x.size()\n",
    "        # Add positional embedding\n",
    "        #\n",
    "        positions = torch.arange(L, device=x.device).unsqueeze(0)  # → (1, L)\n",
    "        pos_emb = self.pos_embedding(positions)      # → (1, L, embed_dim)\n",
    "\n",
    "        #x = x + pos_emb                              # broadcasting over batch \n",
    "            \n",
    "        #print(x.size())\n",
    "       #x = torch.cat(features, dim=1)\n",
    "        x = self.encoder(x) #Transformer Encoder b x 78 x 512\n",
    "        \n",
    "        #x = x.mean(dim=(1), keepdim=False)\n",
    "        x = x[:,0,:]\n",
    "        return self.output_linear(x), x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "668cd433-95dc-4e62-91f8-b5b3ce30446c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BinaryFocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            alpha: weighting factor for class 1 (foreground)\n",
    "            gamma: focusing parameter for modulating factor (1 - p_t)\n",
    "            reduction: 'mean', 'sum', or 'none'\n",
    "        \"\"\"\n",
    "        super(BinaryFocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logits: Tensor of shape (B, 2) – raw output from the model\n",
    "            targets: Tensor of shape (B,) with binary class labels (0 or 1)\n",
    "        \"\"\"\n",
    "        # Convert logits to probabilities\n",
    "        probs = F.softmax(logits, dim=1)  # shape (B, 2)\n",
    "\n",
    "        # Get the probability of the correct class for each example\n",
    "        pt = probs[torch.arange(len(targets)), targets]  # shape (B,)\n",
    "\n",
    "        # Compute the focal loss\n",
    "        alpha_factor = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n",
    "        focal_weight = alpha_factor * (1 - pt) ** self.gamma\n",
    "        loss = -focal_weight * torch.log(pt + 1e-8)\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss  # shape (B,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4d3d7b4f-c349-4b8d-bb85-678960a9daab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 11.23533\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 384\n",
    "num_heads = 8\n",
    "num_layers = 4\n",
    "\n",
    "model = CTPAModel(embed_dim, num_heads, num_layers, drop_prob=0.1)\n",
    "# Alternatively, to just get the total parameter count:\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params/1e6}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04364486-2de1-4c6c-8753-e57e4e3e6804",
   "metadata": {},
   "source": [
    "## Train and Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "46f5fd23-91d3-411b-a8d1-8aee4b8b1c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "def do_test(test_dataloader, model):\n",
    "    test_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    test_losses = []\n",
    "    output = []\n",
    "    with torch.no_grad():\n",
    "        for test_batch in tqdm(test_dataloader, total=len(test_dataloader)):\n",
    "            inputs = test_batch[0] #.to(device)\n",
    "                    \n",
    "            inputs = inputs.to(device)\n",
    "            labels = torch.squeeze(test_batch[1].to(device))\n",
    "           # print(labels)\n",
    "    \n",
    "            outputs,_ = model(inputs) \n",
    "            output.extend(F.softmax(outputs).cpu().numpy()[:, 1])\n",
    "          #  print(outputs)\n",
    "    \n",
    "            \n",
    "            test_loss += criterion(outputs, labels)\n",
    "            \n",
    "            preds = torch.argmax(outputs, dim=1) \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    test_loss /= len(test_dataloader)\n",
    "    test_losses.append(test_loss)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    ba = balanced_accuracy_score(all_labels, all_preds)\n",
    "    f1_scores.append(f1)\n",
    "    confusion_matrix_sc = confusion_matrix(all_labels, all_preds)\n",
    "    tn, fp, fn, tp = confusion_matrix_sc.ravel()\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    all_labels_bin = label_binarize(all_labels, classes=[0, 1]).ravel()\n",
    "    \n",
    "    auc_roc = roc_auc_score(all_labels_bin, output)\n",
    "    auc_pr = average_precision_score(all_labels_bin, output)\n",
    "    metrics = {\n",
    "        'Loss': test_loss / len(test_dataloader.dataset),\n",
    "        'Accuracy': acc,\n",
    "        'Sensitivity': sensitivity,\n",
    "        'Specificity': specificity,\n",
    "        'Precision': precision,\n",
    "        'AUC-ROC': auc_roc,\n",
    "        'AUC-PR': auc_pr,\n",
    "        'Balanced Accuracy': ba,\n",
    "        'F1 Score': f1,\n",
    "    }\n",
    "    print(\"Test loss{b:.3f}, F1 {c:.3f}, Acc {d:.3f}, BA {e:.3f}, precision {f:.3f}, recall {g:.3f}\".format( b=test_loss, c=f1, d = acc, e= ba, f=precision, g=recall))\n",
    "    print(f\"cm{confusion_matrix_sc}\")\n",
    "    return metrics, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "70689b35-059c-4cf8-9785-283e6d6cf2f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\preet\\AppData\\Local\\Temp\\ipykernel_21304\\4024281372.py:28: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 357/357 [00:13<00:00, 27.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Train loss0.486, F1 0.700, Acc 0.767, BA 0.686, precision 0.773, recall 0.438\n",
      "cm[[884  62]\n",
      " [271 211]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\preet\\AppData\\Local\\Temp\\ipykernel_21304\\2734414312.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output.extend(F.softmax(outputs).cpu().numpy()[:, 1])\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 82/82 [00:00<00:00, 107.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss0.471, F1 0.720, Acc 0.799, BA 0.794, precision 0.449, recall 0.786\n",
      "cm[[109  27]\n",
      " [  6  22]]\n",
      "Val AUC-ROC: 0.8534663865546218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 357/357 [00:12<00:00, 28.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n",
      "Train loss0.293, F1 0.871, Acc 0.888, BA 0.860, precision 0.880, recall 0.774\n",
      "cm[[894  51]\n",
      " [109 374]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\preet\\AppData\\Local\\Temp\\ipykernel_21304\\2734414312.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output.extend(F.softmax(outputs).cpu().numpy()[:, 1])\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 82/82 [00:00<00:00, 104.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss0.628, F1 0.696, Acc 0.768, BA 0.789, precision 0.411, recall 0.821\n",
      "cm[[103  33]\n",
      " [  5  23]]\n",
      "Val AUC-ROC: 0.8550420168067226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 357/357 [00:12<00:00, 28.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n",
      "Train loss0.274, F1 0.886, Acc 0.901, BA 0.876, precision 0.898, recall 0.799\n",
      "cm[[901  44]\n",
      " [ 97 386]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\preet\\AppData\\Local\\Temp\\ipykernel_21304\\2734414312.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output.extend(F.softmax(outputs).cpu().numpy()[:, 1])\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 82/82 [00:00<00:00, 104.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss0.592, F1 0.731, Acc 0.805, BA 0.811, precision 0.460, recall 0.821\n",
      "cm[[109  27]\n",
      " [  5  23]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 357/357 [00:12<00:00, 28.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n",
      "Train loss0.281, F1 0.884, Acc 0.900, BA 0.872, precision 0.905, recall 0.786\n",
      "cm[[906  40]\n",
      " [103 379]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\preet\\AppData\\Local\\Temp\\ipykernel_21304\\2734414312.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output.extend(F.softmax(outputs).cpu().numpy()[:, 1])\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 82/82 [00:00<00:00, 104.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss0.570, F1 0.707, Acc 0.780, BA 0.797, precision 0.426, recall 0.821\n",
      "cm[[105  31]\n",
      " [  5  23]]\n",
      "Val AUC-ROC: 0.8555672268907564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 357/357 [00:12<00:00, 28.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n",
      "Train loss0.267, F1 0.896, Acc 0.910, BA 0.884, precision 0.919, recall 0.803\n",
      "cm[[911  34]\n",
      " [ 95 388]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\preet\\AppData\\Local\\Temp\\ipykernel_21304\\2734414312.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output.extend(F.softmax(outputs).cpu().numpy()[:, 1])\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 82/82 [00:00<00:00, 101.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss0.428, F1 0.784, Acc 0.860, BA 0.830, precision 0.564, recall 0.786\n",
      "cm[[119  17]\n",
      " [  6  22]]\n",
      "Val AUC-ROC: 0.8568802521008404\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score,balanced_accuracy_score, precision_score, recall_score\n",
    "import torch\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, RocCurveDisplay\n",
    "import os\n",
    "from sklearn.metrics import average_precision_score\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer (Adam)\n",
    "learning_rate = 1e-5\n",
    "num_epochs = 5\n",
    "model = CTPAModel(embed_dim, num_heads, num_layers,drop_prob=0.2)\n",
    "\n",
    "model = model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR   # or CosineAnnealingWarmRestarts\n",
    "\n",
    "min_lr   = 1e-6                      # floor value\n",
    "scheduler = CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=num_epochs,                # # of scheduler updates before it restarts (here: 1 cycle = whole run)\n",
    "    eta_min=min_lr                   # lowest LR the cosine will hit\n",
    ")\n",
    "use_amp = True\n",
    "device = \"cuda\"\n",
    "thresh = 0\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_rocs=[]\n",
    "test_losses = []\n",
    "test_rocs=[]\n",
    "best = 0\n",
    "for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        total_accuracy = 0.0\n",
    "        num_train_batches = len(train_dataloader)\n",
    "        \n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        f1_scores =[]\n",
    "        \n",
    "        for num_batch,batch in enumerate(tqdm(train_dataloader, total=num_train_batches)):\n",
    "            if num_batch < num_train_batches:\n",
    "                with torch.autocast(device_type=device, dtype=torch.float16, enabled=use_amp):\n",
    "                    inputs = batch[0] #.to(device)\n",
    "                    \n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = torch.squeeze(batch[1].to(device))\n",
    "                   # print(labels)\n",
    "                    #M = (torch.rand(inputs.size()) > thresh).to(torch.long)\n",
    "                    #M = M.cuda()\n",
    "                    #print(M)\n",
    "                    #One = torch.ones(inputs.size()).cuda()\n",
    "                    #inputs = M * inputs + (One-M)*(One-inputs)\n",
    "                    outputs,_ = model(inputs) #.to(torch.long))\n",
    "                  #  print(outputs)\n",
    "    \n",
    "                   # print(outputs.size(), labels.size())\n",
    "                    #print(outputs.shape, labels.shape)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    preds = torch.argmax(outputs, dim=1) \n",
    "                    all_preds.extend(preds.cpu().numpy())\n",
    "                    all_labels.extend(labels.cpu().numpy())\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scheduler.step()\n",
    "                scaler.update()\n",
    "                \n",
    "                optimizer.zero_grad(set_to_none=True) # set_to_none=True here can modestly improve performance\n",
    "                total_loss += loss.item()\n",
    "            #if num_batch == 100:\n",
    "            #    end_timer_and_print(\"Test\")\n",
    "                \n",
    "       # end_timer_and_print(\"Test\")\n",
    "\n",
    "        avg_train_loss = total_loss / num_train_batches\n",
    "        train_losses.append(avg_train_loss)\n",
    "        f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        acc = accuracy_score(all_labels, all_preds)\n",
    "        ba = balanced_accuracy_score(all_labels, all_preds)\n",
    "      \n",
    "        precision = precision_score(all_labels, all_preds)\n",
    "        recall = recall_score(all_labels, all_preds)\n",
    "        f1_scores.append(f1)\n",
    "        confusion_matrix_sc = confusion_matrix(all_labels, all_preds)\n",
    "        print(f'Epoch {epoch + 1}')\n",
    "        print(\"Train loss{b:.3f}, F1 {c:.3f}, Acc {d:.3f}, BA {e:.3f}, precision {f:.3f}, recall {g:.3f}\".format( b=avg_train_loss, c=f1, d = acc, e= ba, f=precision, g=recall))\n",
    "        print(f\"cm{confusion_matrix_sc}\") \n",
    "        model.eval()\n",
    "        \n",
    "        #x = do_test(val_dataloader)\n",
    "        #val_losses.append(x.item())\n",
    "        save_dir = r\"C:\\Users\\preet\\Documents\\penet\\results_sequence\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        # Evaluation\n",
    "        metrics, output = do_test(val_dataloader, model)\n",
    "        roc_auc = metrics[\"AUC-ROC\"]\n",
    "        \n",
    "        # Save if best\n",
    "        if roc_auc >= best:\n",
    "            best = roc_auc\n",
    "            print(\"Val AUC-ROC:\", roc_auc)\n",
    "            val_rocs.append(roc_auc)\n",
    "            fname = os.path.join(save_dir, \"best_model.pth\")\n",
    "            torch.save(model.state_dict(), fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "63e6a815-83b3-46ef-b206-f88042953e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\preet\\AppData\\Local\\Temp\\ipykernel_21304\\2734414312.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output.extend(F.softmax(outputs).cpu().numpy()[:, 1])\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 81/81 [00:00<00:00, 107.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss0.541, F1 0.733, Acc 0.796, BA 0.834, precision 0.455, recall 0.893\n",
      "cm[[104  30]\n",
      " [  3  25]]\n",
      "{'Accuracy': 0.7962962962962963, 'Sensitivity': np.float64(0.8928571428571429), 'Specificity': np.float64(0.7761194029850746), 'Precision': 0.45454545454545453, 'AUC-ROC': 0.9496268656716418, 'AUC-PR': 0.8414195532679931, 'Balanced Accuracy': 0.8344882729211087, 'F1 Score': 0.732740088986652}\n",
      "   Accuracy  Sensitivity  Specificity  Precision   AUC-ROC   AUC-PR  \\\n",
      "0  0.796296     0.892857     0.776119   0.454545  0.949627  0.84142   \n",
      "\n",
      "   Balanced Accuracy  F1 Score  \n",
      "0           0.834488   0.73274  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load(r\"C:\\Users\\preet\\Documents\\penet\\results_sequence\\best_model.pth\") \n",
    "model.load_state_dict(state_dict)\n",
    "metrics, output = do_test(test_dataloader, model)\n",
    "del metrics[\"Loss\"]\n",
    "print(metrics)\n",
    "metrics_df = pd.DataFrame.from_dict([metrics])\n",
    "print(metrics_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "e8abb0bd-b6de-486a-a9c6-4f42b82a7cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metrics(result_df):\n",
    "    from sklearn.metrics import (\n",
    "            accuracy_score, precision_score, recall_score, f1_score,\n",
    "            roc_auc_score, average_precision_score, balanced_accuracy_score,\n",
    "            confusion_matrix\n",
    "        )\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Get true labels and predicted probabilities\n",
    "    y_true = result_df['label'].values\n",
    "    y_prob = result_df['final_prob'].values\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "    \n",
    "    # Confusion matrix to get TN, FP, FN, TP\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    # Compute metrics\n",
    "    metrics = {\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Sensitivity\": recall_score(y_true, y_pred),  # aka recall\n",
    "        \"Specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0.0,\n",
    "        \"Precision\": precision_score(y_true, y_pred),\n",
    "        \"F1 Score\": f1_score(y_true, y_pred),\n",
    "        \"AUC-ROC\": roc_auc_score(y_true, y_prob),\n",
    "        \"AUC-PR\": average_precision_score(y_true, y_prob),\n",
    "        \"Balanced Accuracy\": balanced_accuracy_score(y_true, y_pred)\n",
    "    }\n",
    "        \n",
    "    # Add to metrics_df\n",
    "    metrics_df2 = pd.DataFrame([metrics])  # single row\n",
    "    return metrics_df2 \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
